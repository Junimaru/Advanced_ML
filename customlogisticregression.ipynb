{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class customlogisticregression(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000, regularization_strength=0.01):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.regularization_strength = regularization_strength\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def binary_cross_entropy_loss(self, y_true, y_pred):\n",
    "        # Avoid log(0) which is undefined\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Compute binary cross entropy loss\n",
    "        loss = -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        # Gradient Descent\n",
    "        for _ in range(self.iterations):\n",
    "            # Compute predictions\n",
    "            linear_model = np.dot(X, self.weights) + self.bias\n",
    "            y_predicted = self.sigmoid(linear_model)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = self.binary_cross_entropy_loss(y, y_predicted)\n",
    "            \n",
    "            # Compute gradients\n",
    "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y)) + (self.regularization_strength / n_samples) * self.weights\n",
    "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "            # Print loss\n",
    "            if _ % 100 == 0:\n",
    "                print(f\"Iteration {_}, Loss: {loss}\")\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        linear_model = np.dot(X, self.weights) + self.bias\n",
    "        proba = self.sigmoid(linear_model)\n",
    "        return proba\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return (self.predict_proba(X)>0.5).astype(\"int\")\n",
    "    \n",
    "    def score(self, X, y=None):\n",
    "        predictions = self.predict(X)\n",
    "        accuracy = np.sum(predictions == y) / len(y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkurn\\AppData\\Local\\Temp\\ipykernel_20256\\2760480069.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, Loss: 12.82245531126914\n",
      "Iteration 200, Loss: 14.619638395961367\n",
      "Iteration 300, Loss: 13.878662351560857\n",
      "Iteration 400, Loss: 13.85879440075982\n",
      "Iteration 500, Loss: 13.84162965561909\n",
      "Iteration 600, Loss: 13.83137710015052\n",
      "Iteration 700, Loss: 13.806580485316395\n",
      "Iteration 800, Loss: 13.71403796906074\n",
      "Iteration 900, Loss: 13.662220364368569\n",
      "0.639344262295082\n"
     ]
    }
   ],
   "source": [
    "# Implementation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# import dataset\n",
    "df = pd.read_csv('D:/Advance Machine Learning/heart.csv')\n",
    "df.head(6)\n",
    "\n",
    "# drop missing values\n",
    "df.dropna()\n",
    "\n",
    "# Define X and y\n",
    "X = df.drop('target',axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# split data\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n",
    "\n",
    "# train model with modification parameter\n",
    "model = customlogisticregression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "# predict model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# show accuracy\n",
    "print(accuracy_score(y_test,y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.515966832728044\n",
      "Iteration 200, Loss: 10.764784566239936\n",
      "Iteration 300, Loss: 8.934932413898208\n",
      "Iteration 400, Loss: 8.684803019761922\n",
      "Iteration 500, Loss: 8.611695412749906\n",
      "Iteration 600, Loss: 8.586760185343984\n",
      "Iteration 700, Loss: 8.569804788356231\n",
      "Iteration 800, Loss: 8.554401459381353\n",
      "Iteration 900, Loss: 8.53642635156368\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.219039364484878\n",
      "Iteration 200, Loss: 9.533236841446046\n",
      "Iteration 300, Loss: 8.6040428141707\n",
      "Iteration 400, Loss: 8.280166032389689\n",
      "Iteration 500, Loss: 8.145728638536447\n",
      "Iteration 600, Loss: 8.040953202361207\n",
      "Iteration 700, Loss: 7.962610324832358\n",
      "Iteration 800, Loss: 7.921574908888183\n",
      "Iteration 900, Loss: 7.90425020323281\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 13.260124045835934\n",
      "Iteration 200, Loss: 9.156394032149322\n",
      "Iteration 300, Loss: 8.509407328838902\n",
      "Iteration 400, Loss: 8.451962106167496\n",
      "Iteration 500, Loss: 8.496685753664675\n",
      "Iteration 600, Loss: 8.505289194542001\n",
      "Iteration 700, Loss: 8.503463774201599\n",
      "Iteration 800, Loss: 8.499768173645998\n",
      "Iteration 900, Loss: 8.496261776945614\n",
      "Iteration 0, Loss: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkurn\\AppData\\Local\\Temp\\ipykernel_20256\\2760480069.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, Loss: 15.660801536899903\n",
      "Iteration 200, Loss: 15.334040448676737\n",
      "Iteration 300, Loss: 13.562088051151285\n",
      "Iteration 400, Loss: 13.329374973785844\n",
      "Iteration 500, Loss: 13.221619431988671\n",
      "Iteration 600, Loss: 13.227892988315842\n",
      "Iteration 700, Loss: 13.241943213258706\n",
      "Iteration 800, Loss: 13.250950911586244\n",
      "Iteration 900, Loss: 13.257493316656138\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.446270008997162\n",
      "Iteration 200, Loss: 14.080702735968416\n",
      "Iteration 300, Loss: 13.56359617373094\n",
      "Iteration 400, Loss: 13.056251272176342\n",
      "Iteration 500, Loss: 13.243307015585708\n",
      "Iteration 600, Loss: 13.204774219119324\n",
      "Iteration 700, Loss: 13.134972876280111\n",
      "Iteration 800, Loss: 13.088317754061661\n",
      "Iteration 900, Loss: 13.088786287253438\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.564129922474592\n",
      "Iteration 200, Loss: 13.984133870345701\n",
      "Iteration 300, Loss: 13.34775432756371\n",
      "Iteration 400, Loss: 13.288573384383175\n",
      "Iteration 500, Loss: 13.323275363207964\n",
      "Iteration 600, Loss: 13.345275083734247\n",
      "Iteration 700, Loss: 13.355279851706554\n",
      "Iteration 800, Loss: 13.324336188331216\n",
      "Iteration 900, Loss: 13.349648866248621\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.515966832728044\n",
      "Iteration 200, Loss: 10.764784566239936\n",
      "Iteration 300, Loss: 8.934932413898208\n",
      "Iteration 400, Loss: 8.684803019761922\n",
      "Iteration 500, Loss: 8.611695412749906\n",
      "Iteration 600, Loss: 8.586760185343984\n",
      "Iteration 700, Loss: 8.569804788356231\n",
      "Iteration 800, Loss: 8.554401459381353\n",
      "Iteration 900, Loss: 8.53642635156368\n",
      "Iteration 1000, Loss: 8.519277545915372\n",
      "Iteration 1100, Loss: 8.501850966775446\n",
      "Iteration 1200, Loss: 8.485248174110103\n",
      "Iteration 1300, Loss: 8.467778604321662\n",
      "Iteration 1400, Loss: 8.451481334709962\n",
      "Iteration 1500, Loss: 8.435163317322138\n",
      "Iteration 1600, Loss: 8.41923033460038\n",
      "Iteration 1700, Loss: 8.403396543339491\n",
      "Iteration 1800, Loss: 8.388545699383407\n",
      "Iteration 1900, Loss: 8.373451652141119\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.219039364484878\n",
      "Iteration 200, Loss: 9.533236841446046\n",
      "Iteration 300, Loss: 8.6040428141707\n",
      "Iteration 400, Loss: 8.280166032389689\n",
      "Iteration 500, Loss: 8.145728638536447\n",
      "Iteration 600, Loss: 8.040953202361207\n",
      "Iteration 700, Loss: 7.962610324832358\n",
      "Iteration 800, Loss: 7.921574908888183\n",
      "Iteration 900, Loss: 7.90425020323281\n",
      "Iteration 1000, Loss: 7.896015789018075\n",
      "Iteration 1100, Loss: 7.89215833877156\n",
      "Iteration 1200, Loss: 7.888736995638955\n",
      "Iteration 1300, Loss: 7.885085738069662\n",
      "Iteration 1400, Loss: 7.882503897794246\n",
      "Iteration 1500, Loss: 7.879062085608919\n",
      "Iteration 1600, Loss: 7.87573733311793\n",
      "Iteration 1700, Loss: 7.873384821879841\n",
      "Iteration 1800, Loss: 7.869803009898941\n",
      "Iteration 1900, Loss: 7.8663654966886805\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 13.260124045835934\n",
      "Iteration 200, Loss: 9.156394032149322\n",
      "Iteration 300, Loss: 8.509407328838902\n",
      "Iteration 400, Loss: 8.451962106167496\n",
      "Iteration 500, Loss: 8.496685753664675\n",
      "Iteration 600, Loss: 8.505289194542001\n",
      "Iteration 700, Loss: 8.503463774201599\n",
      "Iteration 800, Loss: 8.499768173645998\n",
      "Iteration 900, Loss: 8.496261776945614\n",
      "Iteration 1000, Loss: 8.492159795865591\n",
      "Iteration 1100, Loss: 8.488448667798274\n",
      "Iteration 1200, Loss: 8.48335325579949\n",
      "Iteration 1300, Loss: 8.477878921596893\n",
      "Iteration 1400, Loss: 8.472851098493724\n",
      "Iteration 1500, Loss: 8.466712158026962\n",
      "Iteration 1600, Loss: 8.46127879015972\n",
      "Iteration 1700, Loss: 8.454196798620455\n",
      "Iteration 1800, Loss: 8.447547196312355\n",
      "Iteration 1900, Loss: 8.440317803243966\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.660801536899903\n",
      "Iteration 200, Loss: 15.334040448676737\n",
      "Iteration 300, Loss: 13.562088051151285\n",
      "Iteration 400, Loss: 13.329374973785844\n",
      "Iteration 500, Loss: 13.221619431988671\n",
      "Iteration 600, Loss: 13.227892988315842\n",
      "Iteration 700, Loss: 13.241943213258706\n",
      "Iteration 800, Loss: 13.250950911586244\n",
      "Iteration 900, Loss: 13.257493316656138\n",
      "Iteration 1000, Loss: 13.262376062695914\n",
      "Iteration 1100, Loss: 13.265431667022991\n",
      "Iteration 1200, Loss: 13.266348166890959\n",
      "Iteration 1300, Loss: 13.26301667234289\n",
      "Iteration 1400, Loss: 13.253371940712025\n",
      "Iteration 1500, Loss: 13.240182588816328\n",
      "Iteration 1600, Loss: 13.225893164534575\n",
      "Iteration 1700, Loss: 13.211218955157843\n",
      "Iteration 1800, Loss: 13.196475864356978\n",
      "Iteration 1900, Loss: 13.181825296628038\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.446270008997162\n",
      "Iteration 200, Loss: 14.080702735968416\n",
      "Iteration 300, Loss: 13.56359617373094\n",
      "Iteration 400, Loss: 13.056251272176342\n",
      "Iteration 500, Loss: 13.243307015585708\n",
      "Iteration 600, Loss: 13.204774219119324\n",
      "Iteration 700, Loss: 13.134972876280111\n",
      "Iteration 800, Loss: 13.088317754061661\n",
      "Iteration 900, Loss: 13.088786287253438\n",
      "Iteration 1000, Loss: 13.092409182889472\n",
      "Iteration 1100, Loss: 13.103085574425757\n",
      "Iteration 1200, Loss: 13.093816976754427\n",
      "Iteration 1300, Loss: 13.10669754110656\n",
      "Iteration 1400, Loss: 13.153262774201382\n",
      "Iteration 1500, Loss: 13.109403138783463\n",
      "Iteration 1600, Loss: 13.143018791147105\n",
      "Iteration 1700, Loss: 13.10446988929863\n",
      "Iteration 1800, Loss: 13.144660330537132\n",
      "Iteration 1900, Loss: 13.165791915212914\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.564129922474592\n",
      "Iteration 200, Loss: 13.984133870345701\n",
      "Iteration 300, Loss: 13.34775432756371\n",
      "Iteration 400, Loss: 13.288573384383175\n",
      "Iteration 500, Loss: 13.323275363207964\n",
      "Iteration 600, Loss: 13.345275083734247\n",
      "Iteration 700, Loss: 13.355279851706554\n",
      "Iteration 800, Loss: 13.324336188331216\n",
      "Iteration 900, Loss: 13.349648866248621\n",
      "Iteration 1000, Loss: 13.264293590552196\n",
      "Iteration 1100, Loss: 13.305826889751923\n",
      "Iteration 1200, Loss: 13.272721116969434\n",
      "Iteration 1300, Loss: 13.305614877358025\n",
      "Iteration 1400, Loss: 13.30031432160542\n",
      "Iteration 1500, Loss: 13.299785901561409\n",
      "Iteration 1600, Loss: 13.368266408812955\n",
      "Iteration 1700, Loss: 13.283266323174395\n",
      "Iteration 1800, Loss: 13.268915201231502\n",
      "Iteration 1900, Loss: 13.379690711062077\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.71939285189542\n",
      "Iteration 200, Loss: 10.264379593749442\n",
      "Iteration 300, Loss: 8.94606897802551\n",
      "Iteration 400, Loss: 8.734603540754316\n",
      "Iteration 500, Loss: 8.679782246643764\n",
      "Iteration 600, Loss: 8.654647495166115\n",
      "Iteration 700, Loss: 8.64047199546065\n",
      "Iteration 800, Loss: 8.63039277118933\n",
      "Iteration 900, Loss: 8.623760038627097\n",
      "Best hyperparameters (RandomizedSearchCV): {'learning_rate': 0.001, 'iterations': 1000}\n",
      "Best score (RandomizedSearchCV): 0.603343621399177\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.515966832728044\n",
      "Iteration 200, Loss: 10.764784566239936\n",
      "Iteration 300, Loss: 8.934932413898208\n",
      "Iteration 400, Loss: 8.684803019761922\n",
      "Iteration 500, Loss: 8.611695412749906\n",
      "Iteration 600, Loss: 8.586760185343984\n",
      "Iteration 700, Loss: 8.569804788356231\n",
      "Iteration 800, Loss: 8.554401459381353\n",
      "Iteration 900, Loss: 8.53642635156368\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.219039364484878\n",
      "Iteration 200, Loss: 9.533236841446046\n",
      "Iteration 300, Loss: 8.6040428141707\n",
      "Iteration 400, Loss: 8.280166032389689\n",
      "Iteration 500, Loss: 8.145728638536447\n",
      "Iteration 600, Loss: 8.040953202361207\n",
      "Iteration 700, Loss: 7.962610324832358\n",
      "Iteration 800, Loss: 7.921574908888183\n",
      "Iteration 900, Loss: 7.90425020323281\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 13.260124045835934\n",
      "Iteration 200, Loss: 9.156394032149322\n",
      "Iteration 300, Loss: 8.509407328838902\n",
      "Iteration 400, Loss: 8.451962106167496\n",
      "Iteration 500, Loss: 8.496685753664675\n",
      "Iteration 600, Loss: 8.505289194542001\n",
      "Iteration 700, Loss: 8.503463774201599\n",
      "Iteration 800, Loss: 8.499768173645998\n",
      "Iteration 900, Loss: 8.496261776945614\n",
      "Iteration 0, Loss: 0.6931471805599453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jkurn\\AppData\\Local\\Temp\\ipykernel_20256\\2760480069.py:10: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 100, Loss: 15.660801536899903\n",
      "Iteration 200, Loss: 15.334040448676737\n",
      "Iteration 300, Loss: 13.562088051151285\n",
      "Iteration 400, Loss: 13.329374973785844\n",
      "Iteration 500, Loss: 13.221619431988671\n",
      "Iteration 600, Loss: 13.227892988315842\n",
      "Iteration 700, Loss: 13.241943213258706\n",
      "Iteration 800, Loss: 13.250950911586244\n",
      "Iteration 900, Loss: 13.257493316656138\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.446270008997162\n",
      "Iteration 200, Loss: 14.080702735968416\n",
      "Iteration 300, Loss: 13.56359617373094\n",
      "Iteration 400, Loss: 13.056251272176342\n",
      "Iteration 500, Loss: 13.243307015585708\n",
      "Iteration 600, Loss: 13.204774219119324\n",
      "Iteration 700, Loss: 13.134972876280111\n",
      "Iteration 800, Loss: 13.088317754061661\n",
      "Iteration 900, Loss: 13.088786287253438\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.564129922474592\n",
      "Iteration 200, Loss: 13.984133870345701\n",
      "Iteration 300, Loss: 13.34775432756371\n",
      "Iteration 400, Loss: 13.288573384383175\n",
      "Iteration 500, Loss: 13.323275363207964\n",
      "Iteration 600, Loss: 13.345275083734247\n",
      "Iteration 700, Loss: 13.355279851706554\n",
      "Iteration 800, Loss: 13.324336188331216\n",
      "Iteration 900, Loss: 13.349648866248621\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.515966832728044\n",
      "Iteration 200, Loss: 10.764784566239936\n",
      "Iteration 300, Loss: 8.934932413898208\n",
      "Iteration 400, Loss: 8.684803019761922\n",
      "Iteration 500, Loss: 8.611695412749906\n",
      "Iteration 600, Loss: 8.586760185343984\n",
      "Iteration 700, Loss: 8.569804788356231\n",
      "Iteration 800, Loss: 8.554401459381353\n",
      "Iteration 900, Loss: 8.53642635156368\n",
      "Iteration 1000, Loss: 8.519277545915372\n",
      "Iteration 1100, Loss: 8.501850966775446\n",
      "Iteration 1200, Loss: 8.485248174110103\n",
      "Iteration 1300, Loss: 8.467778604321662\n",
      "Iteration 1400, Loss: 8.451481334709962\n",
      "Iteration 1500, Loss: 8.435163317322138\n",
      "Iteration 1600, Loss: 8.41923033460038\n",
      "Iteration 1700, Loss: 8.403396543339491\n",
      "Iteration 1800, Loss: 8.388545699383407\n",
      "Iteration 1900, Loss: 8.373451652141119\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.219039364484878\n",
      "Iteration 200, Loss: 9.533236841446046\n",
      "Iteration 300, Loss: 8.6040428141707\n",
      "Iteration 400, Loss: 8.280166032389689\n",
      "Iteration 500, Loss: 8.145728638536447\n",
      "Iteration 600, Loss: 8.040953202361207\n",
      "Iteration 700, Loss: 7.962610324832358\n",
      "Iteration 800, Loss: 7.921574908888183\n",
      "Iteration 900, Loss: 7.90425020323281\n",
      "Iteration 1000, Loss: 7.896015789018075\n",
      "Iteration 1100, Loss: 7.89215833877156\n",
      "Iteration 1200, Loss: 7.888736995638955\n",
      "Iteration 1300, Loss: 7.885085738069662\n",
      "Iteration 1400, Loss: 7.882503897794246\n",
      "Iteration 1500, Loss: 7.879062085608919\n",
      "Iteration 1600, Loss: 7.87573733311793\n",
      "Iteration 1700, Loss: 7.873384821879841\n",
      "Iteration 1800, Loss: 7.869803009898941\n",
      "Iteration 1900, Loss: 7.8663654966886805\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 13.260124045835934\n",
      "Iteration 200, Loss: 9.156394032149322\n",
      "Iteration 300, Loss: 8.509407328838902\n",
      "Iteration 400, Loss: 8.451962106167496\n",
      "Iteration 500, Loss: 8.496685753664675\n",
      "Iteration 600, Loss: 8.505289194542001\n",
      "Iteration 700, Loss: 8.503463774201599\n",
      "Iteration 800, Loss: 8.499768173645998\n",
      "Iteration 900, Loss: 8.496261776945614\n",
      "Iteration 1000, Loss: 8.492159795865591\n",
      "Iteration 1100, Loss: 8.488448667798274\n",
      "Iteration 1200, Loss: 8.48335325579949\n",
      "Iteration 1300, Loss: 8.477878921596893\n",
      "Iteration 1400, Loss: 8.472851098493724\n",
      "Iteration 1500, Loss: 8.466712158026962\n",
      "Iteration 1600, Loss: 8.46127879015972\n",
      "Iteration 1700, Loss: 8.454196798620455\n",
      "Iteration 1800, Loss: 8.447547196312355\n",
      "Iteration 1900, Loss: 8.440317803243966\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.660801536899903\n",
      "Iteration 200, Loss: 15.334040448676737\n",
      "Iteration 300, Loss: 13.562088051151285\n",
      "Iteration 400, Loss: 13.329374973785844\n",
      "Iteration 500, Loss: 13.221619431988671\n",
      "Iteration 600, Loss: 13.227892988315842\n",
      "Iteration 700, Loss: 13.241943213258706\n",
      "Iteration 800, Loss: 13.250950911586244\n",
      "Iteration 900, Loss: 13.257493316656138\n",
      "Iteration 1000, Loss: 13.262376062695914\n",
      "Iteration 1100, Loss: 13.265431667022991\n",
      "Iteration 1200, Loss: 13.266348166890959\n",
      "Iteration 1300, Loss: 13.26301667234289\n",
      "Iteration 1400, Loss: 13.253371940712025\n",
      "Iteration 1500, Loss: 13.240182588816328\n",
      "Iteration 1600, Loss: 13.225893164534575\n",
      "Iteration 1700, Loss: 13.211218955157843\n",
      "Iteration 1800, Loss: 13.196475864356978\n",
      "Iteration 1900, Loss: 13.181825296628038\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.446270008997162\n",
      "Iteration 200, Loss: 14.080702735968416\n",
      "Iteration 300, Loss: 13.56359617373094\n",
      "Iteration 400, Loss: 13.056251272176342\n",
      "Iteration 500, Loss: 13.243307015585708\n",
      "Iteration 600, Loss: 13.204774219119324\n",
      "Iteration 700, Loss: 13.134972876280111\n",
      "Iteration 800, Loss: 13.088317754061661\n",
      "Iteration 900, Loss: 13.088786287253438\n",
      "Iteration 1000, Loss: 13.092409182889472\n",
      "Iteration 1100, Loss: 13.103085574425757\n",
      "Iteration 1200, Loss: 13.093816976754427\n",
      "Iteration 1300, Loss: 13.10669754110656\n",
      "Iteration 1400, Loss: 13.153262774201382\n",
      "Iteration 1500, Loss: 13.109403138783463\n",
      "Iteration 1600, Loss: 13.143018791147105\n",
      "Iteration 1700, Loss: 13.10446988929863\n",
      "Iteration 1800, Loss: 13.144660330537132\n",
      "Iteration 1900, Loss: 13.165791915212914\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 15.564129922474592\n",
      "Iteration 200, Loss: 13.984133870345701\n",
      "Iteration 300, Loss: 13.34775432756371\n",
      "Iteration 400, Loss: 13.288573384383175\n",
      "Iteration 500, Loss: 13.323275363207964\n",
      "Iteration 600, Loss: 13.345275083734247\n",
      "Iteration 700, Loss: 13.355279851706554\n",
      "Iteration 800, Loss: 13.324336188331216\n",
      "Iteration 900, Loss: 13.349648866248621\n",
      "Iteration 1000, Loss: 13.264293590552196\n",
      "Iteration 1100, Loss: 13.305826889751923\n",
      "Iteration 1200, Loss: 13.272721116969434\n",
      "Iteration 1300, Loss: 13.305614877358025\n",
      "Iteration 1400, Loss: 13.30031432160542\n",
      "Iteration 1500, Loss: 13.299785901561409\n",
      "Iteration 1600, Loss: 13.368266408812955\n",
      "Iteration 1700, Loss: 13.283266323174395\n",
      "Iteration 1800, Loss: 13.268915201231502\n",
      "Iteration 1900, Loss: 13.379690711062077\n",
      "Iteration 0, Loss: 0.6931471805599453\n",
      "Iteration 100, Loss: 12.71939285189542\n",
      "Iteration 200, Loss: 10.264379593749442\n",
      "Iteration 300, Loss: 8.94606897802551\n",
      "Iteration 400, Loss: 8.734603540754316\n",
      "Iteration 500, Loss: 8.679782246643764\n",
      "Iteration 600, Loss: 8.654647495166115\n",
      "Iteration 700, Loss: 8.64047199546065\n",
      "Iteration 800, Loss: 8.63039277118933\n",
      "Iteration 900, Loss: 8.623760038627097\n",
      "Best hyperparameters (GridSearchCV): {'iterations': 1000, 'learning_rate': 0.001}\n",
      "Best score (GridSearchCV): 0.603343621399177\n",
      "Accuracy: 0.5573770491803278\n"
     ]
    }
   ],
   "source": [
    "## Hyperparameter TUning\n",
    "param_grid = {\n",
    "    'learning_rate': [0.001, 0.01],\n",
    "    'iterations': [1000, 2000 ]\n",
    "}\n",
    "\n",
    "# RandomizedSearchCV for hyperparameter tuning\n",
    "randomized_cv = RandomizedSearchCV(model, param_distributions=param_grid, cv=3, n_iter=5)\n",
    "randomized_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters (RandomizedSearchCV):\", randomized_cv.best_params_)\n",
    "print(\"Best score (RandomizedSearchCV):\", randomized_cv.best_score_)\n",
    "\n",
    "\n",
    "# GridSearchCV for hyperparameter tuning\n",
    "grid_cv = GridSearchCV(model, param_grid=param_grid, cv=3)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best hyperparameters (GridSearchCV):\", grid_cv.best_params_)\n",
    "print(\"Best score (GridSearchCV):\", grid_cv.best_score_)\n",
    "\n",
    "# Predict using the best model from GridSearchCV\n",
    "best_lr_model = grid_cv.best_estimator_\n",
    "predictions = best_lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = np.sum(predictions == y_test) / len(y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
